{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1df25d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "102dc205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/soniadias/Desktop/classes/text mining for ai /ba-text-mining/lab_sessions/movie_reviews_dataset\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "movie_reviews_folder = cwd.joinpath('movie_reviews_dataset')\n",
    "print('path:', movie_reviews_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "      movie_reviews_folder.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5b17d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 2000\n",
      "neg 1000\n",
      "pos 1000\n"
     ]
    }
   ],
   "source": [
    "# loading all files as training data.\n",
    "movie_reviews_train = load_files(str(movie_reviews_folder))\n",
    "print(\"Length of dataset: \" + str(len(movie_reviews_train.data)))\n",
    "freqs = Counter(movie_reviews_train.target)\n",
    "for category, frequency in freqs.items():\n",
    "    print(movie_reviews_train.target_names[category], frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c247321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soniadias/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "movie_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "movie_counts = movie_vec.fit_transform(movie_reviews_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dea50d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25139\n",
      "['\\x14', '\\x16', '!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'50s\", \"'60s\", \"'70s\", \"'80s\", \"'90\", \"'90s\", \"'94\", \"'96\", \"'97\", \"'98\", \"'being\", \"'bout\", \"'carry\", \"'cause\", \"'comedy\", \"'d\", \"'dark\", \"'do\", \"'em\", \"'end\", \"'funny\", \"'ghetto\", \"'good\", \"'if\", \"'ll\", \"'m\", \"'matron\", \"'n\", \"'new\", \"'no\", \"'normal\", \"'nuff\", \"'psycho\", \"'re\", \"'romeo\", \"'round\", \"'rumble\", \"'s\", \"'secret\", \"'star\", \"'straight\", \"'t\", \"'the\", \"'three\", \"'til\", \"'till\", \"'ve\", \"'what\", \"'when\", \"'you\", '(', ')', '*', '+', '+2', '+3', '+4', ',', '-', '--', '-1', '-4', '-and', '-awarded', '-but', '-ed', '-esque', '-give', '-it', '-jack', '-like', '-lite', '-reviewed', '-ridden', '-so', '-type', '.', '/', '//filmfreakcentral', '//www', '0', '00', '000', '007', '05', '1', '1/10', '1/2', '1/25th', '1/29/97']\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_vec.vocabulary_))\n",
    "print(list(movie_vec.get_feature_names_out())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ee5d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "movie_tfidf = tfidf_transformer.fit_transform(movie_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f6f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the Naive Bayes Classifier\n",
    "\n",
    "# using Multinominal Naive Bayes as model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    movie_tfidf, # the tf-idf model\n",
    "    movie_reviews_train.target, # the category values for each review \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56deb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multimoda Naive Bayes classifier\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdb046d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78818220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8175"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_true=y_test,\n",
    "                             y_pred=y_pred,\n",
    "                             average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82e3295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It took eight years for Warner Brothers to recover from the disaster that was this movie.', 'All the New York University students love this diner in Soho so it makes for a fun young atmosphere.', 'This Italian place is really trendy but they have forgotten about the most important part of a restaurant, the food.', 'In conclusion, my review of this book would be: I like Jane Austen and understand why she is famous.', 'The story of this movie is focused on Carl Brashear played by Cuba Gooding Jr. who wants to be the first African American deep sea diver in the Navy.', \"Chris O'Donnell stated that while filming for this movie, he felt like he was in a toy commercial.\", 'My husband and I moved to Amsterdam 6 years ago and for as long as we have lived here, Blauwbrug has been our favorite place to eat!', 'Dame Maggie Smith performed her role excellently, as she does in all her movies.', 'The new movie by Mr. Kruno was shot in New York, but the story takes place in Los Angeles.', \"I always have loved English novels, but I just couldn't get into this one.\"]\n",
      "['negative', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'negative']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "test_set = []\n",
    "test_set_gold_labels = []\n",
    "\n",
    "with open(\"/Users/soniadias/Desktop/classes/text mining for ai /ba-text-mining/lab_sessions/sentiment-topic-final-test.tsv\") as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for line in tsv_file:\n",
    "        text = line[1]\n",
    "        if text == \"text\":\n",
    "            continue\n",
    "        sentiment = line[2]\n",
    "        test_set.append(text)\n",
    "        test_set_gold_labels.append(sentiment)\n",
    "print(test_set)\n",
    "print(test_set_gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "813b350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25139)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We re-use movie vec to transform it in the same way as the training data\n",
    "new_counts = movie_vec.transform(test_set)\n",
    "new_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13d47342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute tf idf values\n",
    "test_new_tfidf = tfidf_transformer.transform(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05b1444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "pred = clf.predict(test_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7d18b3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: It took eight years for Warner Brothers to recover from the disaster that was this movie. => neg\n",
      "Prediction: It took eight years for Warner Brothers to recover from the disaster that was this movie. => neg\n",
      "Expected: All the New York University students love this diner in Soho so it makes for a fun young atmosphere. => pos\n",
      "Prediction: All the New York University students love this diner in Soho so it makes for a fun young atmosphere. => pos\n",
      "Expected: This Italian place is really trendy but they have forgotten about the most important part of a restaurant, the food. => neg\n",
      "Prediction: This Italian place is really trendy but they have forgotten about the most important part of a restaurant, the food. => pos\n",
      "Expected: In conclusion, my review of this book would be: I like Jane Austen and understand why she is famous. => pos\n",
      "Prediction: In conclusion, my review of this book would be: I like Jane Austen and understand why she is famous. => pos\n",
      "Expected: The story of this movie is focused on Carl Brashear played by Cuba Gooding Jr. who wants to be the first African American deep sea diver in the Navy. => neu\n",
      "Prediction: The story of this movie is focused on Carl Brashear played by Cuba Gooding Jr. who wants to be the first African American deep sea diver in the Navy. => pos\n",
      "Expected: Chris O'Donnell stated that while filming for this movie, he felt like he was in a toy commercial. => neu\n",
      "Prediction: Chris O'Donnell stated that while filming for this movie, he felt like he was in a toy commercial. => pos\n",
      "Expected: My husband and I moved to Amsterdam 6 years ago and for as long as we have lived here, Blauwbrug has been our favorite place to eat! => pos\n",
      "Prediction: My husband and I moved to Amsterdam 6 years ago and for as long as we have lived here, Blauwbrug has been our favorite place to eat! => pos\n",
      "Expected: Dame Maggie Smith performed her role excellently, as she does in all her movies. => pos\n",
      "Prediction: Dame Maggie Smith performed her role excellently, as she does in all her movies. => pos\n",
      "Expected: The new movie by Mr. Kruno was shot in New York, but the story takes place in Los Angeles. => neu\n",
      "Prediction: The new movie by Mr. Kruno was shot in New York, but the story takes place in Los Angeles. => pos\n",
      "Expected: I always have loved English novels, but I just couldn't get into this one. => neg\n",
      "Prediction: I always have loved English novels, but I just couldn't get into this one. => pos\n"
     ]
    }
   ],
   "source": [
    "# model prediction results\n",
    "count = 0\n",
    "for review, predicted_label in zip(test_set, pred):\n",
    "    print('Expected: %s => %s' % (test_set[count], \n",
    "                        test_set_gold_labels[count][0:3]))\n",
    "    count += 1\n",
    "    print('Prediction: %s => %s' % (review, \n",
    "                        movie_reviews_train.target_names[predicted_label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253e3ee",
   "metadata": {},
   "source": [
    "Error Analysis: \n",
    "The model correctly identified 5 out of the 10 sentences in the test set. Of the incorrectly identified sentences, 3 of the 5 of them were originally labeled as \"neutral.\" The training data that was used, a Cornell Movie Reviews dataset, did not have a \"neutral\" labelled subsection which accounts for 60% of the error in predictions. If the model did not have a section in which to learn how to predict neutral sentences, there is no way that it would be able to label any neutral sentences. All neutral sentences were are classified as positive, most likely because there were no explicitly negative sentiments expressed. In the future, I would include neutral labeling in the training data.  \n",
    "The other two incorrectly labeled sentences were both compound sentences in which the first part of the sentence is positive, but the second part is negative using the conjuction \"but.\" It is possible that the model did not know how to encounter partially positive and partially negative sentences or was not trained with sentences with a negation in the middle. Regardless, this is a type of sentence that should be included more frequently in the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
