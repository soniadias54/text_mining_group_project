{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2d8898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = '/Users/maddie/Downloads/ner.csv'\n",
    "training_data = pd.read_csv(path, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee0676d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a2b09de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lessen the amount of data so the computer can handle it; caused kernel to repeatedly crash otherwise\n",
    "\n",
    "training_data = training_data.head(5000)\n",
    "words = list(set(training_data[\"Sentence\"].values))\n",
    "n_words = len(words); n_words\n",
    "pos = list(set(training_data[\"POS\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8c1d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tags used in the training data\n",
    "\n",
    "labels = list(set(training_data[\"Tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7cfdb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "label_counts = collections.Counter(list(training_data[\"Tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73359f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Sentence\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a3a3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9345f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .', \"['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']\", \"['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\")]\n"
     ]
    }
   ],
   "source": [
    "sent = getter.get_next()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0758fab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .', \"['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'CC', 'VB', 'DT', 'NN', 'IN', 'JJ', 'NNS', 'IN', 'DT', 'NN', '.']\", \"['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\")]\n"
     ]
    }
   ],
   "source": [
    "# ensure that the sentences were split properly\n",
    "sentences = getter.sentences\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "879c1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    print(word)\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    # data structure consisting of a feature name and value for the token\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(), # lower case variant of the token\n",
    "        'word[-3:]': word[-3:], #suffix of 3 characters\n",
    "        'word[-2:]': word[-2:], #suffix of 2 characters\n",
    "        'word.isupper()': word.isupper(), # initial captial\n",
    "        'word.istitle()': word.istitle(), # all words ini caps\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2], #first two characters of the PoS Tag\n",
    "    }\n",
    "    if i > 0:\n",
    "        # adding features for the word based on the previous word\n",
    "        word1 = sent[i-1][0] # previous word\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # Beginning of sentence as a feature\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        # adding features for the word based on the next word\n",
    "        word1 = sent[i+1][0] # next word\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # end of sentence as a feature\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "931532e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "took\n",
      "eight\n",
      "years\n",
      "for\n",
      "Warner\n",
      "Brothers\n",
      "to\n",
      "recover\n",
      "from\n",
      "the\n",
      "disaster\n",
      "that\n",
      "was\n",
      "this\n",
      "movie\n",
      ".\n",
      "All\n",
      "the\n",
      "New\n",
      "York\n",
      "University\n",
      "students\n",
      "love\n",
      "this\n",
      "diner\n",
      "in\n",
      "Soho\n",
      "so\n",
      "it\n",
      "makes\n",
      "for\n",
      "a\n",
      "fun\n",
      "young\n",
      "atmosphere\n",
      ".\n",
      "This\n",
      "Italian\n",
      "place\n",
      "is\n",
      "really\n",
      "trendy\n",
      "but\n",
      "they\n",
      "have\n",
      "forgotten\n",
      "about\n",
      "the\n",
      "most\n",
      "important\n",
      "part\n",
      "of\n",
      "a\n",
      "restaurant\n",
      ",\n",
      "the\n",
      "food\n",
      ".\n",
      "In\n",
      "conclusion\n",
      ",\n",
      "my\n",
      "review\n",
      "of\n",
      "this\n",
      "book\n",
      "would\n",
      "be\n",
      ":\n",
      "I\n",
      "like\n",
      "Jane\n",
      "Austen\n",
      "and\n",
      "understand\n",
      "why\n",
      "she\n",
      "is\n",
      "famous\n",
      ".\n",
      "The\n",
      "story\n",
      "of\n",
      "this\n",
      "movie\n",
      "is\n",
      "focused\n",
      "on\n",
      "Carl\n",
      "Brashear\n",
      "played\n",
      "by\n",
      "Cuba\n",
      "Gooding\n",
      "Jr.\n",
      "who\n",
      "wants\n",
      "to\n",
      "be\n",
      "the\n",
      "first\n",
      "African\n",
      "American\n",
      "deep\n",
      "sea\n",
      "diver\n",
      "in\n",
      "the\n",
      "Navy\n",
      ".\n",
      "Chris\n",
      "O'Donnell\n",
      "stated\n",
      "that\n",
      "while\n",
      "filming\n",
      "for\n",
      "this\n",
      "movie\n",
      ",\n",
      "he\n",
      "felt\n",
      "like\n",
      "he\n",
      "was\n",
      "in\n",
      "a\n",
      "toy\n",
      "commercial\n",
      ".\n",
      "My\n",
      "husband\n",
      "and\n",
      "I\n",
      "moved\n",
      "to\n",
      "Amsterdam\n",
      "6\n",
      "years\n",
      "ago\n",
      "and\n",
      "for\n",
      "as\n",
      "long\n",
      "as\n",
      "we\n",
      "have\n",
      "lived\n",
      "here\n",
      ",\n",
      "Blauwbrug\n",
      "has\n",
      "been\n",
      "our\n",
      "favorite\n",
      "place\n",
      "to\n",
      "eat\n",
      "!\n",
      "Dame\n",
      "Maggie\n",
      "Smith\n",
      "performed\n",
      "her\n",
      "role\n",
      "excellently\n",
      ",\n",
      "as\n",
      "she\n",
      "does\n",
      "in\n",
      "all\n",
      "her\n",
      "movies\n",
      ".\n",
      "The\n",
      "new\n",
      "movie\n",
      "by\n",
      "Mr.\n",
      "Kruno\n",
      "was\n",
      "shot\n",
      "in\n",
      "New\n",
      "York\n",
      ",\n",
      "but\n",
      "the\n",
      "story\n",
      "takes\n",
      "place\n",
      "in\n",
      "Los\n",
      "Angeles\n",
      ".\n",
      "I\n",
      "always\n",
      "have\n",
      "loved\n",
      "English\n",
      "novels\n",
      ",\n",
      "but\n",
      "I\n",
      "just\n",
      "could\n",
      "n't\n",
      "get\n",
      "into\n",
      "this\n",
      "one\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# define X and y for the training data\n",
    "# once again limit the data set so as to prevent crashes\n",
    "\n",
    "X = [sent2features(s) for s in sentences[:1000]]\n",
    "y = [sent2labels(s) for s in sentences[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e62e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'it', 'word[-3:]': 'It', 'word[-2:]': 'It', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', 'BOS': True, '+1:word.lower()': 'took', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'took', 'word[-3:]': 'ook', 'word[-2:]': 'ok', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'it', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'eight', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'eight', 'word[-3:]': 'ght', 'word[-2:]': 'ht', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'took', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'years', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'years', 'word[-3:]': 'ars', 'word[-2:]': 'rs', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'eight', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'for', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'for', 'word[-3:]': 'for', 'word[-2:]': 'or', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'years', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'warner', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'B-ORG', '+1:postag[:2]': 'B-'}, {'bias': 1.0, 'word.lower()': 'warner', 'word[-3:]': 'ner', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'B-ORG', 'postag[:2]': 'B-', '-1:word.lower()': 'for', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'brothers', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'I-ORG', '+1:postag[:2]': 'I-'}, {'bias': 1.0, 'word.lower()': 'brothers', 'word[-3:]': 'ers', 'word[-2:]': 'rs', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'I-ORG', 'postag[:2]': 'I-', '-1:word.lower()': 'warner', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'B-ORG', '-1:postag[:2]': 'B-', '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'brothers', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'I-ORG', '-1:postag[:2]': 'I-', '+1:word.lower()': 'recover', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'recover', 'word[-3:]': 'ver', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'from', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'from', 'word[-3:]': 'rom', 'word[-2:]': 'om', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'recover', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'the', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'from', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'disaster', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'disaster', 'word[-3:]': 'ter', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'the', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'that', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'that', 'word[-3:]': 'hat', 'word[-2:]': 'at', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'disaster', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'was', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'was', 'word[-3:]': 'was', 'word[-2:]': 'as', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'that', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'this', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'this', 'word[-3:]': 'his', 'word[-2:]': 'is', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'was', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'movie', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'movie', 'word[-3:]': 'vie', 'word[-2:]': 'ie', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'this', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'movie', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', 'EOS': True}]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ab663cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "# initialize the model\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1, .\n",
    "          c2=0.1,\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c32c21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f1d96cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the training data\n",
    "\n",
    "try:\n",
    "    crf.fit(X, y)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ed66270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence id</th>\n",
       "      <th>token id</th>\n",
       "      <th>token</th>\n",
       "      <th>BIO NER tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>It</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>took</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>eight</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>years</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence id  token id  token BIO NER tag\n",
       "0            0         0     It           O\n",
       "1            0         1   took           O\n",
       "2            0         2  eight           O\n",
       "3            0         3  years           O\n",
       "4            0         4    for           O"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload the test data\n",
    "\n",
    "test_path = '/Users/maddie/Downloads/NER-final-test.tsv'\n",
    "test_data = pd.read_table(test_path, on_bad_lines='skip')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ece5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep', 'has', 'diver', 'Gooding', 'Chris', 'as', 'was', 'The', 'new', 'Smith', 'conclusion', 'in', 'he', 'husband', 'wants', 'be', 'commercial', 'all', 'review', 'All', 'been', 'she', 'my', 'lived', 'movie', 'famous', 'York', 'young', 'first', 'Los', 'atmosphere', 'they', 'I', 'played', 'one', 'Cuba', 'My', 'her', 'sea', 'Mr.', 'place', 'Kruno', 'Blauwbrug', 'part', 'In', 'moved', 'but', 'here', 'performed', 'recover', 'is', 'could', 'story', 'focused', 'that', 'excellently', 'forgotten', 'loved', 'into', 'It', 'diner', 'and', 'by', 'Navy', 'our', 'students', 'a', 'University', ':', 'we', 'this', 'English', 'Jr.', 'Carl', 'Angeles', 'movies', 'felt', 'novels', 'get', 'Amsterdam', 'restaurant', 'would', 'it', 'from', 'filming', 'eat', 'just', 'so', 'really', 'eight', 'Austen', 'African', \"n't\", 'Soho', 'Jane', 'Italian', 'important', '.', 'makes', 'disaster', 'who', 'Maggie', ',', 'on', 'understand', 'like', 'does', 'American', 'of', 'This', 'love', 'years', 'why', 'trendy', 'the', 'favorite', 'for', 'Brashear', '6', 'long', 'shot', 'always', 'have', 'book', 'while', 'Dame', 'New', 'Warner', 'stated', 'fun', 'most', 'Brothers', 'took', \"O'Donnell\", 'ago', 'toy', 'role', 'takes', 'about', 'to', '!', 'food']\n",
      "214\n",
      "['O', 'B-MISC', 'B-LOC', 'I-PER', 'I-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG']\n"
     ]
    }
   ],
   "source": [
    "# observe the words and tags of the test data set\n",
    "\n",
    "words = list(set(test_data[\"token\"].values))\n",
    "print(words)\n",
    "n_words = len(test_data); n_words\n",
    "print(n_words)\n",
    "pos = list(set(test_data[\"BIO NER tag\"].values))\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "496e1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine SentenceGetter to fit the indices of the test data\n",
    "\n",
    "class SentenceGetterTest(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"token\"].values.tolist(),\n",
    "                                                     s[\"BIO NER tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence id\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "be64dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_getter = SentenceGetterTest(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "478b3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test_getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c63ae5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'O'), ('conclusion', 'O'), (',', 'O'), ('my', 'O'), ('review', 'O'), ('of', 'O'), ('this', 'O'), ('book', 'O'), ('would', 'O'), ('be', 'O'), (':', 'O'), ('I', 'O'), ('like', 'O'), ('Jane', 'B-PER'), ('Austen', 'I-PER'), ('and', 'O'), ('understand', 'O'), ('why', 'O'), ('she', 'O'), ('is', 'O'), ('famous', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3ee218cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine word2features to match the format of test_data\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    print(word)\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    # data structure consisting of a feature name and value for the token\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(), # lower case variant of the token\n",
    "        'word[-3:]': word[-3:], #suffix of 3 characters\n",
    "        'word[-2:]': word[-2:], #suffix of 2 characters\n",
    "        'word.isupper()': word.isupper(), # initial captial\n",
    "        'word.istitle()': word.istitle(), # all words ini caps\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2], #first two characters of the PoS Tag\n",
    "    }\n",
    "    if i > 0:\n",
    "        # adding features for the word based on the previous word\n",
    "        word1 = sent[i-1][0] # previous word\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # Beginning of sentence as a feature\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        # adding features for the word based on the next word\n",
    "        word1 = sent[i+1][0] # next word\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # end of sentence as a feature\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "03fc1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "took\n",
      "eight\n",
      "years\n",
      "for\n",
      "Warner\n",
      "Brothers\n",
      "to\n",
      "recover\n",
      "from\n",
      "the\n",
      "disaster\n",
      "that\n",
      "was\n",
      "this\n",
      "movie\n",
      ".\n",
      "All\n",
      "the\n",
      "New\n",
      "York\n",
      "University\n",
      "students\n",
      "love\n",
      "this\n",
      "diner\n",
      "in\n",
      "Soho\n",
      "so\n",
      "it\n",
      "makes\n",
      "for\n",
      "a\n",
      "fun\n",
      "young\n",
      "atmosphere\n",
      ".\n",
      "This\n",
      "Italian\n",
      "place\n",
      "is\n",
      "really\n",
      "trendy\n",
      "but\n",
      "they\n",
      "have\n",
      "forgotten\n",
      "about\n",
      "the\n",
      "most\n",
      "important\n",
      "part\n",
      "of\n",
      "a\n",
      "restaurant\n",
      ",\n",
      "the\n",
      "food\n",
      ".\n",
      "In\n",
      "conclusion\n",
      ",\n",
      "my\n",
      "review\n",
      "of\n",
      "this\n",
      "book\n",
      "would\n",
      "be\n",
      ":\n",
      "I\n",
      "like\n",
      "Jane\n",
      "Austen\n",
      "and\n",
      "understand\n",
      "why\n",
      "she\n",
      "is\n",
      "famous\n",
      ".\n",
      "The\n",
      "story\n",
      "of\n",
      "this\n",
      "movie\n",
      "is\n",
      "focused\n",
      "on\n",
      "Carl\n",
      "Brashear\n",
      "played\n",
      "by\n",
      "Cuba\n",
      "Gooding\n",
      "Jr.\n",
      "who\n",
      "wants\n",
      "to\n",
      "be\n",
      "the\n",
      "first\n",
      "African\n",
      "American\n",
      "deep\n",
      "sea\n",
      "diver\n",
      "in\n",
      "the\n",
      "Navy\n",
      ".\n",
      "Chris\n",
      "O'Donnell\n",
      "stated\n",
      "that\n",
      "while\n",
      "filming\n",
      "for\n",
      "this\n",
      "movie\n",
      ",\n",
      "he\n",
      "felt\n",
      "like\n",
      "he\n",
      "was\n",
      "in\n",
      "a\n",
      "toy\n",
      "commercial\n",
      ".\n",
      "My\n",
      "husband\n",
      "and\n",
      "I\n",
      "moved\n",
      "to\n",
      "Amsterdam\n",
      "6\n",
      "years\n",
      "ago\n",
      "and\n",
      "for\n",
      "as\n",
      "long\n",
      "as\n",
      "we\n",
      "have\n",
      "lived\n",
      "here\n",
      ",\n",
      "Blauwbrug\n",
      "has\n",
      "been\n",
      "our\n",
      "favorite\n",
      "place\n",
      "to\n",
      "eat\n",
      "!\n",
      "Dame\n",
      "Maggie\n",
      "Smith\n",
      "performed\n",
      "her\n",
      "role\n",
      "excellently\n",
      ",\n",
      "as\n",
      "she\n",
      "does\n",
      "in\n",
      "all\n",
      "her\n",
      "movies\n",
      ".\n",
      "The\n",
      "new\n",
      "movie\n",
      "by\n",
      "Mr.\n",
      "Kruno\n",
      "was\n",
      "shot\n",
      "in\n",
      "New\n",
      "York\n",
      ",\n",
      "but\n",
      "the\n",
      "story\n",
      "takes\n",
      "place\n",
      "in\n",
      "Los\n",
      "Angeles\n",
      ".\n",
      "I\n",
      "always\n",
      "have\n",
      "loved\n",
      "English\n",
      "novels\n",
      ",\n",
      "but\n",
      "I\n",
      "just\n",
      "could\n",
      "n't\n",
      "get\n",
      "into\n",
      "this\n",
      "one\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# use the sentences of the test data to obtain X_test and y_test\n",
    "\n",
    "X_test = [sent2features(s) for s in sentences]\n",
    "y_test = [sent2labels(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7b7cf6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'it', 'word[-3:]': 'It', 'word[-2:]': 'It', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', 'BOS': True, '+1:word.lower()': 'took', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'took', 'word[-3:]': 'ook', 'word[-2:]': 'ok', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'it', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'eight', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'eight', 'word[-3:]': 'ght', 'word[-2:]': 'ht', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'took', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'years', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'years', 'word[-3:]': 'ars', 'word[-2:]': 'rs', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'eight', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'for', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'for', 'word[-3:]': 'for', 'word[-2:]': 'or', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'years', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'warner', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'B-ORG', '+1:postag[:2]': 'B-'}, {'bias': 1.0, 'word.lower()': 'warner', 'word[-3:]': 'ner', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'B-ORG', 'postag[:2]': 'B-', '-1:word.lower()': 'for', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'brothers', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'I-ORG', '+1:postag[:2]': 'I-'}, {'bias': 1.0, 'word.lower()': 'brothers', 'word[-3:]': 'ers', 'word[-2:]': 'rs', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'I-ORG', 'postag[:2]': 'I-', '-1:word.lower()': 'warner', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'B-ORG', '-1:postag[:2]': 'B-', '+1:word.lower()': 'to', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'to', 'word[-3:]': 'to', 'word[-2:]': 'to', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'brothers', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'I-ORG', '-1:postag[:2]': 'I-', '+1:word.lower()': 'recover', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'recover', 'word[-3:]': 'ver', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'to', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'from', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'from', 'word[-3:]': 'rom', 'word[-2:]': 'om', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'recover', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'the', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'from', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'disaster', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'disaster', 'word[-3:]': 'ter', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'the', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'that', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'that', 'word[-3:]': 'hat', 'word[-2:]': 'at', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'disaster', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'was', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'was', 'word[-3:]': 'was', 'word[-2:]': 'as', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'that', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'this', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'this', 'word[-3:]': 'his', 'word[-2:]': 'is', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'was', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': 'movie', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': 'movie', 'word[-3:]': 'vie', 'word[-2:]': 'ie', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'this', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'O', '+1:postag[:2]': 'O'}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'O', 'postag[:2]': 'O', '-1:word.lower()': 'movie', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'O', '-1:postag[:2]': 'O', 'EOS': True}]\n"
     ]
    }
   ],
   "source": [
    "# ensure that the X_test data looks correct\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c7b6cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction based on the test data\n",
    "\n",
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d1c230cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'I-ORG',\n",
       "  'I-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'I-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'O'],\n",
       " ['B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-ORG',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['B-PER',\n",
       "  'I-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PER',\n",
       "  'I-PER',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bdfe01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision-score: 100.0%\n",
      "recall-score: 100.0%\n",
      "F1-score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# calculate precision, recall, and F1 based on the predictions and the actual\n",
    "print(\"precision-score: {:.1%}\".format(precision_score(y_test, predictions)))\n",
    "print(\"recall-score: {:.1%}\".format(recall_score(y_test, predictions)))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dcc07c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       1.00      1.00      1.00         4\n",
      "        MISC       1.00      1.00      1.00         3\n",
      "         ORG       1.00      1.00      1.00         4\n",
      "         PER       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        17\n",
      "   macro avg       1.00      1.00      1.00        17\n",
      "weighted avg       1.00      1.00      1.00        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make a report based on the above\n",
    "\n",
    "print(classification_report(y, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
